{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaeaU5mofkDO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA400ZbVf9T7"
      },
      "outputs": [],
      "source": [
        "\n",
        "words = open('names.txt','r').read().splitlines()\n",
        "\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO6GrCikf9ZJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "\n",
        "print(itos)\n",
        "print(vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiGimqYJgzjb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlvUnVrqhMtd"
      },
      "outputs": [],
      "source": [
        "block = 3\n",
        "\n",
        "def build_dataset(words):\n",
        "\n",
        "\n",
        "\n",
        "  X, Y = [], []\n",
        "  for w in words:\n",
        "    context = [0] * block\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix]\n",
        "\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape,Y.shape)\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UJ7vjd_hjbl"
      },
      "outputs": [],
      "source": [
        "for x,y in zip(Xtr[:20],Ytr[:20]):\n",
        "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf6LQu4Gi2K2"
      },
      "outputs": [],
      "source": [
        "# Neural Net\n",
        "\n",
        "\n",
        "class Linear:\n",
        "  def __init__(self,fan_in,fan_out,bias=True):\n",
        "    self.weight = torch.randn((fan_in,fan_out))/fan_in**0.5\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self,x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self,dim,eps=1e-5,momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # Trainable parameters using backprop\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # Buffers\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self,x):\n",
        "\n",
        "    if self.training:\n",
        "      xmean = x.mean(0,keepdim=True)\n",
        "      xvar = x.var(0,keepdim=True)\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "\n",
        "    xhat = (x-xmean)/torch.sqrt(xvar+self.eps)\n",
        "    self.out = self.gamma*xhat + self.beta\n",
        "\n",
        "    # Updating Buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1-self.momentum)*self.runnning_mean + self.momentum*xmean\n",
        "        self.running_var = (1-self.momentum)*self.runnning_var + self.momentum*xvar\n",
        "\n",
        "    return self.out\n",
        "\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma,self.beta]\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "\n",
        "  def __call__(self,x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------\n",
        "\n",
        "class Embedding:\n",
        "\n",
        "  def __init__(self,num_embeddings,embedding_dim):\n",
        "    self.weight = torch.randn((num_embeddings,embedding_dim))\n",
        "\n",
        "  def __call__(self,IX):\n",
        "    self.out = self.weight[IX]\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------\n",
        "\n",
        "class Flatten:\n",
        "  def __call__(self,x):\n",
        "    self.out = x.view(x.shape[0],-1)\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "# -------------------------------------------------------------------------------------------\n",
        "\n",
        "class Sequential:\n",
        "\n",
        "  def __init__(self,layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [p for layer in self.layers for p in layer.parameters()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b5gmhlqnS6f"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EPRBltWl3xY"
      },
      "outputs": [],
      "source": [
        "n_emb = 10 # dimensionality of character embedding vectors\n",
        "n_hidden = 200 # no of neurons in hidden layers of MLP\n",
        "\n",
        "# C = torch.randn((vocab_size,n_emb))\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size,n_emb),\n",
        "    Flatten(),\n",
        "    Linear(n_emb*block,n_hidden,bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "    Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1 # scaled down for better initialization of network\n",
        "\n",
        "parameters = model.parameters()\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKU0YwgunRjy"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  ix = torch.randint(0,Xtr.shape[0],(batch_size,))\n",
        "  Xb,Yb = Xtr[ix],Ytr[ix]\n",
        "\n",
        "  # forward pass\n",
        "    # emb = C[Xb]\n",
        "    # x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb)\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update (stochastic gradient descent)\n",
        "  lr = 0.1 if i<150000 else 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr*p.grad\n",
        "\n",
        "  # tracking stats\n",
        "  if i % 10000 == 0:\n",
        "    print(f'{i:7d}/{max_steps:7d} : {loss.item():.4f}')\n",
        "\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWNXeeUGq1fj"
      },
      "outputs": [],
      "source": [
        "plt.plot(torch.tensor(lossi).view(-1,1000).mean(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6d53Os5p8ji"
      },
      "outputs": [],
      "source": [
        "# put layers into evaluation mode\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33tVhe7fqG-p"
      },
      "outputs": [],
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "      'train': (Xtr,Ytr),\n",
        "      'val' : (Xdev,Ydev),\n",
        "      'test': (Xte,Yte),\n",
        "  }[split]\n",
        "\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits,y)\n",
        "  print(split,loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6z5dyAbq7Fw"
      },
      "outputs": [],
      "source": [
        "# Sampling from the model\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "  out = []\n",
        "  context = [0]*block\n",
        "\n",
        "  while True:\n",
        "    # emb = C[torch.tensor([context])] # (1,block_size,n_emb)\n",
        "    # x = emb.view(emb.shape[0], -1) # concatenating vectors\n",
        "    logits = model(torch.tensor([context]))\n",
        "    probs = F.softmax(logits,dim = 1)\n",
        "\n",
        "    ix = torch.multinomial(probs,num_samples=1).item()\n",
        "\n",
        "    context = context[1:] + [ix]\n",
        "    out.append(ix)\n",
        "\n",
        "    if ix == 0:\n",
        "      break\n",
        "\n",
        "  print(''.join(itos[i] for i in out))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNOpXuUX/9gi1AVoGayiHXZ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
